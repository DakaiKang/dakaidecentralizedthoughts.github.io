---
title: Consensus for State Machine Replication
date: 2019-10-15 22:58:00 -04:00
tags:
- dist101
author: Kartik Nayak, Ittai Abraham, Ling Ren
---

We introduced definitions for consensus, Byzantine Broadcast (BB) and Byzantine Agreement (BA), in an [earlier post](https://decentralizedthoughts.github.io/2019-06-27-defining-consensus/). In this post, we discuss how consensus protocols are used in **State Machine Replication** ([SMR](https://en.wikipedia.org/wiki/State_machine_replication)), a fundamental approach in distributed computing for building fault-tolerant systems. We compare and contrast this setting to that of traditional BB and BA. A [follow-up post](https://decentralizedthoughts.github.io/2022-11-19-from-single-shot-to-smr/) discusses the reductions from one abstraction to the other in the omission failure model.

(Note: the definitions and discussion below are updated in October 2024 to improve rigor and clarity.) 


### State machine

Let's start with the definition of a state machine. A state machine, at any point, stores a *state* of the system. It receives *inputs* (also referred to as *commands*). The state machine applies these inputs in a *sequential order* using a deterministic *transition function* to generate an *output* and an *updated* state. A succinct description of the state machine is as follows:

```
state = init
log = []
while true:
  on receiving cmd:
    log.append(cmd)
    state, output = apply(cmd, state)
```

Here, the state machine is initialized to an initial state `init`. When it receives an input `cmd`, it first adds the input to a `log`. It then *executes* the `cmd` by applying the transition function `apply` to the state. As a result, it obtains an updated state and an output `output`. 

In a client-server setting, the server maintains the state machine, and clients send commands to the server. The output is then sent back to the client.

An example state machine is the Bitcoin ledger. The state consists of the set of public keys along with the associated Bitcoins (see [UTXO](https://www.mycryptopedia.com/bitcoin-utxo-unspent-transaction-output-set-explained/)). The input (or cmd) to the state machine is a transaction (see [Bitcoin core api](https://bitcoin.org/en/developer-reference#bitcoin-core-apis)). The log corresponds to the Bitcoin ledger. The transition function `apply` is the function that determines whether a transaction is valid, and if it is, performs the desired bitcoin script operation (see [script](https://en.bitcoin.it/wiki/Script)).

We start with the notion of multi-shot consensus.

### Multi-shot consensus - a server centric definition


In this problem a set of servers maintain a dynamically growing log of commands. We assume these commands arrive as input to the servers over time (presumably from clients but we abstract that away).

Let $log_i[s]$ be the $s$-th entry in the log of replica $i$. Initially, $log_i[s]=\bot$ for all $s$ and $i$. Replica $i$ writes each $log_i[s]$ *only once* to a value that is not $\bot$. 

**Safety:** Non-faulty replicas agree on each log entry. More precisely, if two non-faulty replicas $i$ and $j$ have $log_i[s] \neq \bot$ and $log_j[s] \neq \bot$, then $log_i[s] = log_j[s]$.

**Liveness:** Every input that arrives to a non-faulty server is eventually recorded in the log. 

**Validity:** There is an *injective mapping* between every entry $log_i[s] \neq \bot$ and the set of inputs. So it cannot be that a log entry does not originate from an input or that two log entries originate from a single input. See below, as many systems may have additional external validity properties.
  
**Prefix completeness:** If a non-faulty replica $i$ has $log_i[s] \neq \bot$, then for all non-faulty replica $j$ and all indexes $s' \le s$, eventually $log_j[s'] \neq \bot$. 


#### Comparison to single shot consensus, broadcast and other notes

Note that safety and agreement are similar in multi-shot and single-shot.  However, there are some differences:

1. **Consensus on a sequence of values.** Conceptually, one can sequentially compose single-shot consensus protocol instances. In practice, there are better designs than sequential composition (more discussed below and in future posts).

2. **Fault tolerance.** An essential consequence of clients learning the results without participating in the protocol is the best fault tolerance one can hope for. With broadcast, we know of protocols such as Dolev-Strong that can tolerate $f < n-1$ Byzantine faults among $n$ replicas. SMR protocols can tolerate at most minority Byzantine faults.

3. **External validity.** Consensus protocols typically also satisfy *external validity*, i.e., validity that is left to the application. Some systems may require that any command that makes it into the log pass some application-level validity checks or be generated by the right client (e.g., in the right format or signed by the client who owns the bitcoin). Other systems are more "liberal" when adding commands to the log and choose to weed out invalid commands at the execution stage. 
 
4. **Fairness.** The liveness requirement above only guarantees that each input is *eventually* committed but does not say how two different inputs need to be ordered relative to each other. In some protocol some stronger degree of fairness on the ordering of commands. We plan to expand on this in future posts and this is deeply connected to challenges in MEV.

We now move to a client centric view of the world for state machine replication and show how multi-shot consensus can be used to implement state machine replication. 


### State machine replication - a clint centric definition

Here we assume that in addition to servers (some of which may be corrupted) there is a set of clients. We assume that any number of clients can have omission failures and in the byzantine model may also allow client to have malicious corruptions.

Let $L$ be a log of consecutive commands and $SM(L)$ is the state machine after applying this sequence of commands. We also assume that the state machine returns an output as part of its new state, denoted as $out$ and sends that back to the client as a response. The requirements are:

**SMR Liveness**: If a non-faulty client issues a *request* then it eventually gets a *response*. 

* Note that this definition is the analog of the *Liveness* property in the above section. It depends on both the Liveness and Prefix completeness.

**SMR Safety**: If two requests return outputs $out_1$ and $out_2$ then there are two logs $L_1$ and $L_2$ such that: one is a prefix of the other, $out_1$ is the output of $SM(L_1)$, and $out_2$ is the output of $SM(L_2)$.

* Note that this definition is the analog of the *Safety* property in the above section. 

**SMR Validity**: The request returns $out$ which is the output of some $SM(L)$ and each value in $L$ can be mapped uniquely to a client request.

* Note that this definition is the analog of the *Validity* property in the above section.

**SMR Correctness**: For a request with value $cmd$, its response, and any response from a request that started after this response for $cmd$ arrives, returns the output of some $SM(L)$ such that $L$ includes $cmd$.

* Note that this property has no analogue as it is a consistency property between the client, not the servers. There may be variants of this property that provide weaker consistency between clients, or slightly stronger properties that aim to simulate an ideal functionality (see [this post](https://decentralizedthoughts.github.io/2021-10-16-the-ideal-state-machine-model-multiple-clients-and-linearizability/)). 

See our [follow-up post](https://decentralizedthoughts.github.io/2022-11-19-from-single-shot-to-smr/) for more about this definition.

Observe that given the server centric definitions what is missing to implement the client centric definitions is a client protocol that maintains the SMR Correctness property. We plan to discuss this in future posts.


### From multi-shot consensus to state machine replication

The clients send requests to the servers. Once a server sees a request it uses it as its input.

The servers run a multi-shot consensus protocol. 

A server executes a command $cmd \neq \bot$, if all the previous entries in it log are non-$\bot$ and have been executed as well.

A server returns a response to the client once it executes it.


#### Proof

SMR Liveness follows from  Liveness and Prefix completeness

SMR Safety follows directly from Safety

SMR Validity follows directly from Validity

For SMR Correctness, consider a response for $cmd$ that is placed in $\log[k]$ then we know that all log entries from 1 to k are full. So clearly any request that starts after this response must be allocated to a higher slot. Hence from SMR Safety, this new response will see $\log[k]=cmd$ as required.


### Optimizing for a sequence of values

Since FT-SMR protocols agree on a sequence of values, practical approaches for SMR (such as [PBFT](http://pmg.csail.mit.edu/papers/osdi99.pdf), [Paxos](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf), etc.) use a steady-state-and-view-change approach to design log replication. In the steady state, there is a designated leader that drives consensus. Typically, the leader does not change until it fails to make progress (e.g., due to network delays) or if Byzantine behavior is detected. In either case, the replicas vote to de-throne the leader and elect a new one. The process of choosing a new leader is called view-change. The stable leader for extended periods yields simplicity and efficiency when the leader is honest. However, it also reduces the amount of *decentralization* and can cause delays if Byzantine replicas are elected as leaders.

### Separation of concerns

The process of adding a new command to a FT-SMR can be decomposed into three tasks:

1. Disseminating the command 
2. Committing the command
3. Executing the command  

Many modern FT-SMR systems have separate sub-systems for each task. This allows each task to work as a separate system in parallel. Each sub-system can optimize for parallelization, can have a separate buffer of incoming requests, and can stream tasks to the next sub-system. Separating into sub-systems allows to optimize and tune each one and to better detect bottlenecks. See [this post](https://decentralizedthoughts.github.io/2019-12-06-dce-the-three-scalability-bottlenecks-of-state-machine-replication/) for the basics of SMR task separation and [this post](https://decentralizedthoughts.github.io/2022-06-28-DAG-meets-BFT/) for the modern separation of the data dissemination stage. 


### Acknowledgments

Thanks to [Maxwill](https://twitter.com/tensorfi) and Julian Loss for suggestions to improve the definition.

Please leave comments on [Twitter](https://twitter.com/kartik1507/status/1185321750881538050?s=20)
